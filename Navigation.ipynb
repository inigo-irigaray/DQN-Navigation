{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Navigation\n",
    "\n",
    "---\n",
    "\n",
    "You are welcome to use this coding environment to train your agent for the project.  Follow the instructions below to get started!\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "Run the next code cells to install a few packages.  This line will take a few minutes to run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install ./python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorboardX==1.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Set the environment ready\n",
    "\n",
    "Run the next code cells to import the necessary packages and files to run the environment and train the agent. Check if any GPU is available for training and set the hyperparameters for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import model\n",
    "import n_step\n",
    "import helpers\n",
    "import replaybuffer\n",
    "import agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 0.99\n",
    "LEARNING_RATE = 1e-4\n",
    "BATCH_SIZE = 64\n",
    "STOP_REWARD = 13.0\n",
    "\n",
    "SEED = 1\n",
    "\n",
    "TARGET_NET_SYNC = 1000\n",
    "\n",
    "STEP_COUNTS = 2\n",
    "\n",
    "REPLAY_SIZE = 100000\n",
    "REPLAY_INITIAL = 10000\n",
    "\n",
    "QUANT = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Start training your agent!\n",
    "\n",
    "All set! Run the main cell below to train your agent in the environment!\n",
    "(Sidenote: You might need to update the path to the Banana environment file to initialize it depending on where you installed it)\n",
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    env = UnityEnvironment(file_name=\"/data/Banana_Linux_NoVis/Banana.x86_64\")\n",
    "    print(\"environment loaded\")\n",
    "    brain_name = env.brain_names[0]\n",
    "    brain = env.brains[brain_name]\n",
    "    env_info = env.reset(train_mode=True)[brain_name]\n",
    "    \n",
    "    torch.manual_seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    \n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    writer = SummaryWriter()\n",
    "\n",
    "    net = model.RainbowDQN(len(env_info.vector_observations[0]), brain.vector_action_space_size,\n",
    "                           QUANT).to(device)\n",
    "    print(net)\n",
    "    tgt_net = model.TargetNet(net)\n",
    "    agent = agent.Agent(net, device=device)\n",
    "    \n",
    "    exp_source = n_step.ExperienceSourceFirstLast(env, agent, GAMMA, STEP_COUNTS, brain_name)\n",
    "    buffer = replaybuffer.ReplayBuffer(REPLAY_SIZE, exp_source)\n",
    "    optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE)\n",
    "    \n",
    "    frame_idx = 0\n",
    "    with helpers.RewardTracker(writer, STOP_REWARD) as reward_tracker:\n",
    "        while True:\n",
    "            frame_idx += 1\n",
    "            buffer.populate(1)\n",
    "            \n",
    "            new_rewards = exp_source.pop_total_rewards()\n",
    "            if new_rewards:\n",
    "                if reward_tracker.reward(new_rewards[0], frame_idx):\n",
    "                    torch.save(tgt_net.target_model.state_dict(), 'checkpoint.pth')\n",
    "                    break\n",
    "            \n",
    "            if len(buffer) < REPLAY_INITIAL:\n",
    "                continue\n",
    "                \n",
    "            optimizer.zero_grad()\n",
    "            batch = buffer.sample(BATCH_SIZE)\n",
    "            loss_v = helpers.calc_loss(batch, net, tgt_net.target_model, QUANT,\n",
    "                                       GAMMA**STEP_COUNTS, device=device)\n",
    "            loss_v.backward()\n",
    "            optimizer.step()\n",
    "            if frame_idx % TARGET_NET_SYNC == 0:\n",
    "                tgt_net.sync()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
